{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FindPhrases.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4Jp33u/15G+/gZTfQ/S+L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sujata018/NLP/blob/main/FindPhrases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlefkuSwRtmO"
      },
      "source": [
        "import nltk.corpus\n",
        "from nltk.collocations import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1FcY3LrSjbo",
        "outputId": "cbae4600-8c9c-4fdd-cb6e-436f34b226b4"
      },
      "source": [
        "nltk.download('abc')              # download corpus abc\n",
        "nltk.corpus.abc.words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/abc.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aROiLh3VGbp",
        "outputId": "0784e395-320f-455c-f925-73956b1f3066"
      },
      "source": [
        "'''\n",
        "Finds phrases with two words, based on frequency of occurring and pmi.\n",
        "\n",
        "PMI(x,y) is the conditional probability of x occurring after y, given y has already occurred.\n",
        "\n",
        "'''\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()            # for calculating pmi (Pointwise Mutual Information)\n",
        "\n",
        "bgFinder=BigramCollocationFinder.from_words(nltk.corpus.abc.words()) # find bigrams\n",
        "bgFinder.apply_freq_filter(500)                                      # discard bigrams that appear < 500 times\n",
        "bgFinder.nbest(bigram_measures.pmi, 10)                              # display 10 bigrams with the best pmi scores\n",
        "\n",
        "'''\n",
        "It lists down all stopwords like \".\",\"'\",\",\" etc. Only valid bigrams are 'per cent', 'he said', 'has been', 'have been' in the output.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('per', 'cent'),\n",
              " ('he', 'said'),\n",
              " (',\"', 'he'),\n",
              " ('\"', 'We'),\n",
              " ('has', 'been'),\n",
              " (\"'\", 's'),\n",
              " (\"'\", 've'),\n",
              " (\"'\", 't'),\n",
              " (\"'\", 're'),\n",
              " ('have', 'been')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN0yIQ_oM1X_",
        "outputId": "b8c020e7-916c-4ad4-9f0f-e9911d0fc082"
      },
      "source": [
        "nltk.download('stopwords')            # Download stopwords, so they can be excluded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7NjjotEM9p4",
        "outputId": "834d861e-8f96-4f12-852f-423981fe12b7"
      },
      "source": [
        "'''\n",
        "same code as above, excluding the stop word as well. \n",
        "Frequency filter is rediced from 500 to 50, as number of bigrams reduced drastically \n",
        "after discarding stopwords.\n",
        "'''\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "\n",
        "bgFinder=BigramCollocationFinder.from_words(nltk.corpus.abc.words())\n",
        "bgFinder.apply_freq_filter(50)   # discard bigrams that appeared < 50 times in corpus                                    \n",
        "\n",
        "ignored_words = nltk.corpus.stopwords.words('english') # get list of stopwords in English language\n",
        "bgFinder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words) # words with lenght < 3 and the stopwords are discarded (this excludes 'I', 'we' etc.)\n",
        "bgFinder.nbest(bigram_measures.pmi, 10)\n",
        "\n",
        "'''\n",
        "The output bigrams are valid phrases.\n",
        "So, next, extend the length of the phrases to 3, 4 etc.\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Primary', 'Industries'),\n",
              " ('United', 'States'),\n",
              " ('Northern', 'Territory'),\n",
              " ('single', 'desk'),\n",
              " ('chief', 'executive'),\n",
              " ('Prime', 'Minister'),\n",
              " ('Farmers', 'Federation'),\n",
              " ('Cole', 'inquiry'),\n",
              " ('Peter', 'McGauran'),\n",
              " ('journal', 'Nature')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "2_fkGivmTcFx",
        "outputId": "f7501b02-20ca-40d8-a05c-3af4b1697952"
      },
      "source": [
        "from operator import itemgetter\n",
        "from nltk.metrics.association import QuadgramAssocMeasures\n",
        "\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "fourgram_measures = QuadgramAssocMeasures()\n",
        "\n",
        "bgFinder=BigramCollocationFinder.from_words(nltk.corpus.abc.words())\n",
        "bgFinder.apply_freq_filter(50)   # discard bigrams that appeared < 50 times in corpus                                    \n",
        "\n",
        "tgFinder=TrigramCollocationFinder.from_words(nltk.corpus.abc.words())\n",
        "tgFinder.apply_freq_filter(30)   # discard trigrams that appeared < 30 times in corpus                                    \n",
        "\n",
        "fgFinder=QuadgramCollocationFinder.from_words(nltk.corpus.abc.words())\n",
        "fgFinder.apply_freq_filter(10)   # discard quadgrams that appeared < 10 times in corpus                                    \n",
        "\n",
        "ignored_words = nltk.corpus.stopwords.words('english') # get list of stopwords in English language\n",
        "bgFinder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words) # words with lenght < 3 and the stopwords are discarded (this excludes 'I', 'we' etc.)\n",
        "tgFinder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words) # words with lenght < 3 and the stopwords are discarded (this excludes 'I', 'we' etc.)\n",
        "fgFinder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words) # words with lenght < 3 and the stopwords are discarded (this excludes 'I', 'we' etc.)\n",
        "\n",
        "d=[(a,b) for a,b in bgFinder.ngram_fd.items() or tgFinder.ngram_fd.items() or fgFinder.ngram_fd.items() ] # concatenate all phrases in a single list\n",
        "unique_phrases=[]      # This list will have unique phrases, e.g. 'South Wales', and 'New South Wales' are both identified, 'South Wales' will be discarded\n",
        "\n",
        "for i in range(len(d)): # check all phrases\n",
        "  if d[i][0] not in list( map(itemgetter(0), d[i+1:] )): # if the phrase is not part of any other phrases, then identify as unique \n",
        "    unique_phrases.append(d[i])\n",
        "print(sorted(unique_phrases, key=lambda t: (-t[1], t[0]))[:20])\n",
        "\n",
        "'''\n",
        "20 most frequent valid phrases identified, with two, three or four words\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('per', 'cent'), 555), (('New', 'South'), 421), (('South', 'Wales'), 421), (('Federal', 'Government'), 322), (('years', 'ago'), 283), (('Western', 'Australia'), 231), (('South', 'Australia'), 211), (('researchers', 'say'), 211), (('New', 'Zealand'), 177), (('last', 'year'), 164), (('climate', 'change'), 150), (('million', 'years'), 150), (('single', 'desk'), 136), (('Northern', 'Territory'), 132), (('scientists', 'say'), 131), (('first', 'time'), 123), (('Farmers', 'Federation'), 98), (('journal', 'Nature'), 95), (('next', 'year'), 93), (('Association', 'says'), 91)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n20 most frequent valid phrases identified, with two, three or four words\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}