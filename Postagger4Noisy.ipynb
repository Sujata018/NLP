{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Postagger4Noisy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZiRD7AolqMES2F4Y3hID3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sujata018/NLP/blob/main/Postagger4Noisy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4bwNB230hU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6da5f10-4e0a-45d0-8c3c-d465e27be33d"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown    # importing brown corpus to be used as English Dictionary of words\n",
        "\n",
        "#nltk.download('brown')\n",
        "#nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgGojWQ9XVs5"
      },
      "source": [
        "dictWords=set() # Vocabulary in a global set \n",
        "dic={}          # Global dictionary to store root (only non repeating consonants) pronunciation of all dictionary words\n",
        "trigrams={}     # Global dictionary to store trigram probabilities\n",
        "bigrams={}      # Global dictionary to store bigram probabilities\n",
        "unigrams={}     # Global dictionary to store frequencies of individual words in the vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-vj0X3CQKnC"
      },
      "source": [
        "def get_consonants(word):\n",
        "  '''\n",
        "  Given a word in the input, this function lists down the consonants in order\n",
        "  of their appearence, removing the repeating ones.\n",
        "  e.g.  'arrogance' will return 'rgnc'\n",
        "\n",
        "  Use: To find a dictionary word phonetically similar to a noisy word,\n",
        "  consonant orders can be matched.\n",
        "  e.g. 'arognce' -> 'rgnc' -> 'arrogance'\n",
        "\n",
        "  '''\n",
        "  word=word.lower()\n",
        "  root=''             # variable to store consonant sequence\n",
        "  ## First check for consonant sequence excluding h,w,y\n",
        "  for letter in word:\n",
        "    if (letter not in 'aeiouhwy'):\n",
        "      if len(root)==0:\n",
        "        root += letter\n",
        "      elif root[-1] != letter:\n",
        "        root += letter  \n",
        "\n",
        "  ## In case no consonants, i.e. the word is made of vowels and h,w,y,\n",
        "  ## then check for appearence sequence of h,w,y\n",
        "  if len(root) == 0:    # The word is built with aeiouhwy only\n",
        "    for letter in word:\n",
        "      if (letter not in 'aeiou'):\n",
        "        if len(root)==0:\n",
        "          root += letter\n",
        "        elif root[-1] != letter:\n",
        "          root += letter  \n",
        "    \n",
        "  return root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text,stopwords):\n",
        "  '''\n",
        "  Tokenise text into sentences and words\n",
        "  '''\n",
        "  tokens=[]        # stores list of sentences. Each sentence is stored as a list of words.\n",
        "  sentence=[]      # temporarily stores list of words in a single sentence\n",
        "  word=''          # temporarily stores a word\n",
        "  L=len(text)\n",
        "\n",
        "  i=0\n",
        "  while i < L :\n",
        "    if text[i] in stopwords:        # if end of a sentence reached\n",
        "      if word != '':\n",
        "        sentence += [word]\n",
        "        word = ''\n",
        "        \n",
        "      if sentence != []:\n",
        "        sentence += [text[i]]\n",
        "        tokens += [sentence]        # add the sentence to tokens\n",
        "      sentence=[]\n",
        "      word=''\n",
        "      while((i<L) and \n",
        "            ((text[i] in stopwords) or \n",
        "            (text[i]==' '))):       # Skip any subsequent stopword(s) and blank spaces\n",
        "          i += 1\n",
        "      i -= 1 \n",
        "    elif text[i] == ' ':            # end of a word\n",
        "      if word != '':\n",
        "        sentence += [word]          # add word to the sentence\n",
        "        word = ''\n",
        "      while(i < L and \n",
        "            text[i]==' '):          # Skip any subsequent blank spaces\n",
        "        i += 1\n",
        "      i -= 1\n",
        "    else:                           # beginning or continuation of a word\n",
        "      word += text[i]               # add letter to the word\n",
        "    \n",
        "    i += 1\n",
        "  \n",
        "  return tokens "
      ],
      "metadata": {
        "id": "1AzXRA00HJ79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildDic():\n",
        "  '''\n",
        "  Build a dictionary of root pronunciations from English Dictionary words in brown corpora\n",
        "  '''\n",
        "  global dic, dictWords\n",
        "  \n",
        "  # Get the list of English words in lower case \n",
        "  dictWords1=set([a.lower() for a in brown.words()])\n",
        "  \n",
        "  # Remove all single letter words (excluding valid words 'a' and 'i'), as the brown corpora has all letters in it.\n",
        "  dictWords=set([a for a in dictWords1 if (len(a)>1 and \"'\" not in a) or (a in 'ai')])\n",
        "\n",
        "  # Build a dictionary of root pronunciations from English Dictionary words\n",
        "  for word in dictWords:\n",
        "    root=get_consonants(word)\n",
        "    if root in dic.keys():\n",
        "      dic[root] += [word]\n",
        "    else:\n",
        "      dic[root] = [word]"
      ],
      "metadata": {
        "id": "USR5MHRPQoZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildProbDistBrown():\n",
        "  '''\n",
        "  Build trigram and bigram and unigram frequencies from Brown corpus\n",
        "  '''\n",
        "  global unigrams, bigrams, trigrams\n",
        "\n",
        "  brown_sentences=brown.sents()\n",
        "  unigrams={}\n",
        "  bigrams={}\n",
        "  trigrams={}\n",
        "\n",
        "  word2=''                         # stores the previous to previous word\n",
        "  word1='<START>'                  # stores the previous word \n",
        "  for sentence in brown_sentences:\n",
        "    for word in sentence:\n",
        "      word=word.lower()\n",
        "      if word in unigrams.keys():  # stores unigram count\n",
        "        unigrams[word] += 1\n",
        "      else:\n",
        "        unigrams[word] = 1\n",
        "      if word1=='<START>':         # stores unigram count for <START>\n",
        "        if word1 in unigrams.keys():\n",
        "          unigrams[word1] += 1\n",
        "        else:\n",
        "          unigrams[word1] = 1\n",
        "      if word1!='':                # stores bigram count\n",
        "        bigram=(word1,word)\n",
        "        if bigram in bigrams.keys():\n",
        "          bigrams[bigram] += 1\n",
        "        else:\n",
        "          bigrams[bigram] = 1\n",
        "      if word2!='':                # stores trigram count\n",
        "        trigram=(word2,word1,word)\n",
        "        if trigram in trigrams.keys():\n",
        "          trigrams[trigram] += 1\n",
        "        else:\n",
        "          trigrams[trigram] = 1\n",
        "\n",
        "      word2=word1                 # push read words back in sequence to read the next word            \n",
        "      word1=word\n",
        "    \n",
        "    word2=''                       # initialise previous words at the start of every sentence\n",
        "    word1='<START>'\n",
        "\n",
        "  #Calculate trigram probabilities as frequency of the third word, given the 1st and the 2nd word.\n",
        "  for trigram in trigrams.keys():\n",
        "    trigrams[trigram] /= bigrams[trigram[:2]]\n",
        "\n",
        "  #Calculate bigram probabilities as frequency of the second word, given the 1st word.\n",
        "  for bigram in bigrams.keys():\n",
        "    bigrams[bigram] /= unigrams[bigram[0]] \n",
        "\n",
        "  #Calculate unigram probabilities as frequency of the word, among all words in the vocabulary.\n",
        "  C=len(unigrams)\n",
        "  for unigram in unigrams.keys():\n",
        "    unigrams[unigram] /= C "
      ],
      "metadata": {
        "id": "SDHsEegNV-mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5OiuaffXH3R"
      },
      "source": [
        "def guessWord(sentence,i):\n",
        "  '''\n",
        "  Given a non-dictionary word, this function guesses the most probable \n",
        "  dictionary word.\n",
        "  Inputs: sentence - the context of the word\n",
        "          i        - the location where the word (to be guessed) is present\n",
        "  Output: word     - the guessed word based on maximum probability\n",
        "  Logic:\n",
        "    Get the consonant sequence of the noisy word. If there is a single \n",
        "    word in dictionary having same consonant sequence (i.e. similarly \n",
        "    pronounced), then return that word. \n",
        "    If there are multiple words, then guess the word by maximising trigram probability\n",
        "       using a context of last two words in the sentence.\n",
        "       In case the incorrect word is in the first or second position of the sentence,\n",
        "       or if no words found with non-zero trigram probability, then use bigram \n",
        "       probability with last word or previous word as <START>.\n",
        "  '''\n",
        "  global dic, trigrams, bigrams\n",
        "  \n",
        "  word=sentence[i]\n",
        "  root=get_consonants(word)       \n",
        "  \n",
        "  if root in dic.keys():\n",
        "    if len(dic[root])==1:\n",
        "      return dic[root][0]\n",
        "    else:\n",
        "      ## Select the most probable word from dic[root] ##\n",
        "      maxProb=0\n",
        "      if i > 0:                           # if the word is in 2nd or later position,  \n",
        "        for cword in dic[root]:           #   check trigram probability of each possibility \n",
        "          if i > 1:\n",
        "            trigram=(sentence[i-2],sentence[i-1],cword) \n",
        "          else:\n",
        "            trigram=('<START>',sentence[i-1],cword)\n",
        "          if trigram in trigrams.keys():\n",
        "            if trigrams[trigram]>maxProb: # pick the possible word with maximum probability\n",
        "              word=cword\n",
        "              maxProb=trigrams[trigram]\n",
        "      if maxProb==0 and word==sentence[i]:# In case trigram not found or it is the first word\n",
        "        for cword in dic[root]:   \n",
        "          if i == 0:                      #   check bigram probability\n",
        "            bigram=('<START>',cword)\n",
        "          else:\n",
        "            bigram=(sentence[i-1],cword)\n",
        "          if bigram in bigrams.keys():\n",
        "            if bigrams[bigram]>maxProb:   # pick the possible word with maximum probability\n",
        "              word=cword\n",
        "              maxProb=bigrams[bigram]\n",
        "  return word  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak_EpAQW0rcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db7b3153-cec3-429f-daa4-55eff572323c"
      },
      "source": [
        "global dic,bigrams,trigrams\n",
        "\n",
        "text=\"hw r yu ? i m gud, yu? What's up? so so.\"\n",
        "stopwords='?,.;'\n",
        "\n",
        "# Tokenise input noisy text\n",
        "noisy_text_tokenized=tokenize(text,stopwords)  \n",
        "\n",
        "# Build a dictionary of root pronunciations from English Dictionary words in brown corpus\n",
        "buildDic()    \n",
        "\n",
        "# Build unigram, bigram and trigram probabilities from brown corpus\n",
        "buildProbDistBrown()\n",
        "\n",
        "# Correct the noisy text using the dictionary of pronunciations.\n",
        "# If any word is not present in English Dictionary, guess the word using\n",
        "# bigram and trigram probabilities, and correct\n",
        "\n",
        "corrected_text=''                           # Corrected text to be stored here\n",
        "C_text_tokenized=[x[:] for x in noisy_text_tokenized]\n",
        "                                            # Deepcopy of noisy tokenised text\n",
        "                                            # the tokenised text will be corrected here\n",
        "for sentence in C_text_tokenized:           # check each sentence in noisy text\n",
        "  for i in range(len(sentence)):            #  check each word\n",
        "    word= sentence[i].lower()\n",
        "    if word not in dictWords and word not in stopwords:\n",
        "      word=guessWord(sentence,i)            # if word is not a dictionary word, then guess using trigram, bigram probabilities\n",
        "      sentence[i]=word                      # correct the word\n",
        "    corrected_text += (word + ' ')    \n",
        "\n",
        "# Apply POS TAGger on the noisy and corrected texts and print for verification\n",
        "a=[nltk.pos_tag(sentence) for sentence in noisy_text_tokenized]\n",
        "c=[nltk.pos_tag(sentence) for sentence in C_text_tokenized]\n",
        "\n",
        "print('noisy_text=',text)\n",
        "print('POS TAGs by nltk',a)\n",
        "print()\n",
        "print('corrected_text=',corrected_text)\n",
        "print('POS TAGs by nltk',c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noisy_text= hw r yu ? i m gud, yu? What's up? so so.\n",
            "POS TAGs by nltk [[('hw', 'NN'), ('r', 'NN'), ('yu', 'NN'), ('?', '.')], [('i', 'JJ'), ('m', 'NN'), ('gud', 'NN'), (',', ',')], [('yu', 'NN'), ('?', '.')], [(\"What's\", 'VB'), ('up', 'RP'), ('?', '.')], [('so', 'RB'), ('so', 'RB'), ('.', '.')]]\n",
            "\n",
            "corrected_text= how are you ? i am gud , you ? What's up ? so so . \n",
            "POS TAGs by nltk [[('how', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('?', '.')], [('i', 'NN'), ('am', 'VBP'), ('gud', 'NN'), (',', ',')], [('you', 'PRP'), ('?', '.')], [(\"What's\", 'VB'), ('up', 'RP'), ('?', '.')], [('so', 'RB'), ('so', 'RB'), ('.', '.')]]\n"
          ]
        }
      ]
    }
  ]
}