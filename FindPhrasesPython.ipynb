{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FindPhrasesPython.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMI+0U9NY77wq+xM8jxuQgh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sujata018/NLP/blob/main/FindPhrasesPython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW2SHxo0cMPs",
        "outputId": "fb88be27-66ca-4a69-efb3-f1976db1bde8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]   Package abc is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import abc\n",
        "\n",
        "nltk.download('abc')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fgrams={}       # Global dictionary to store fourgram counts\n",
        "trigrams={}     # Global dictionary to store trigram counts\n",
        "bigrams={}      # Global dictionary to store bigram counts\n",
        "uniquegrams={}  # Global repository for all unique grams and their counts"
      ],
      "metadata": {
        "id": "wOgCqd0QcpO2"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildGramCount():\n",
        "  '''\n",
        "  Build fgram, trigram and bigram counts from abc corpus\n",
        "  '''\n",
        "  global bigrams, trigrams, fgrams\n",
        "\n",
        "  sentences=abc.sents()\n",
        "  bigrams={}\n",
        "  trigrams={}\n",
        "  fgrams={}\n",
        "\n",
        "  word3=''                    # previous to previous to previous word \n",
        "  word2=''                    # stores the previous to previous word\n",
        "  word1=''                    # stores the previous word \n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      if word in ' !.,:?\"\\'':\n",
        "        continue\n",
        "      else:\n",
        "        word=word.lower()\n",
        "        if word1!='':         # stores bigram count\n",
        "          bigram=(word1,word)\n",
        "          if bigram in bigrams.keys():\n",
        "            bigrams[bigram] += 1\n",
        "          else:\n",
        "            bigrams[bigram] = 1\n",
        "        if word2!='':         # stores trigram count\n",
        "          trigram=(word2,word1,word)\n",
        "          if trigram in trigrams.keys():\n",
        "            trigrams[trigram] += 1\n",
        "          else:\n",
        "            trigrams[trigram] = 1\n",
        "        if word3!='':         # stores fgram count\n",
        "          fgram=(word3,word2,word1,word)\n",
        "          if fgram in fgrams.keys():\n",
        "            fgrams[fgram] += 1\n",
        "          else:\n",
        "            fgrams[fgram] = 1\n",
        "\n",
        "        word3=word2           # push word sequence back before reading next word\n",
        "        word2=word1                             \n",
        "        word1=word\n",
        "    \n",
        "    word3=''                  # initialise word sequence at sentence starting\n",
        "    word2=''                       \n",
        "    word1=''\n"
      ],
      "metadata": {
        "id": "-9gW-9Xoc8ST"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createUniqueGrams():\n",
        "  '''\n",
        "  Merge the dictionaries for bigram, trigram and fourgrams and remove subgrams,\n",
        "  i.e. if a bigram is already part of a trigram, then remove the bigram.\n",
        "  '''\n",
        "  global uniquegrams,bigrams,trigrams,fgrams\n",
        "\n",
        "  uniquegrams={}\n",
        "\n",
        "  for bigram in bigrams.keys():           # Add all bigrams to uniquegrams\n",
        "      uniquegrams[bigram]=bigrams[bigram]   \n",
        " \n",
        "  for trigram in trigrams.keys():         # If any bigram is part of a trigram \n",
        "    if trigram[:2] in uniquegrams.keys(): #  then delete the bigram\n",
        "      del uniquegrams[trigram[:2]]\n",
        "    if trigram[1:] in uniquegrams.keys():\n",
        "      del uniquegrams[trigram[1:]]    \n",
        "  for trigram in trigrams.keys():         # Add all trigrams to uniquegrams\n",
        "      uniquegrams[trigram]=trigrams[trigram]  \n",
        " \n",
        "  for fgram in fgrams.keys():             # If any trigram is part of a fgram \n",
        "    if fgram[:3] in uniquegrams.keys():   #  then delete the trigram\n",
        "      del uniquegrams[fgram[:3]]\n",
        "    if fgram[1:] in uniquegrams.keys():\n",
        "      del uniquegrams[fgram[1:]]    \n",
        "  for fgram in fgrams.keys():             # copy all fourgrams to uniquegrams\n",
        "    uniquegrams[fgram]=fgrams[fgram]"
      ],
      "metadata": {
        "id": "o5vsi_j9gsWv"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This program prints the most frequest 20 phrases from the English 'abc' corpus.\n",
        "'''\n",
        "\n",
        "global uniquegrams,bigrams,trigrams,fgrams\n",
        "\n",
        "# Count bigrams, trigrams, fourgrams from corpus\n",
        "buildGramCount()\n",
        "\n",
        "# Keep counts of the unique phrases only\n",
        "createUniqueGrams()\n",
        "\n",
        "# Print 20 most frequent phrases\n",
        "print((sorted(uniquegrams.items(),key=lambda kv : kv[1],reverse=True))[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JkoSxMBkn_z",
        "outputId": "98bdc807-6fbf-4ff2-c358-be631c7e223b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('at', 'the', 'university', 'of'), 182), (('issue', 'of', 'the', 'journal'), 119), (('of', 'the', 'university', 'of'), 115), (('from', 'the', 'university', 'of'), 109), (('in', 'new', 'south', 'wales'), 85), (('the', 'end', 'of', 'the'), 82), (('for', 'the', 'first', 'time'), 80), (('the', 'new', 'south', 'wales'), 74), (('today', 'in', 'the', 'journal'), 69), (('in', 'the', 'journal', 'nature'), 64), (('the', 'federal', 'government', 'has'), 63), (('agriculture', 'minister', 'peter', 'mcgauran'), 62), (('oil', '-', 'for', '-'), 60), (('-', 'for', '-', 'food'), 60), (('-', 'year', '-', 'old'), 60), (('of', 'new', 'south', 'wales'), 60), (('per', 'cent', 'of', 'the'), 59), (('meat', 'and', 'livestock', 'australia'), 58), (('in', 'the', 'state', 's'), 57), (('to', 'be', 'able', 'to'), 55)]\n"
          ]
        }
      ]
    }
  ]
}